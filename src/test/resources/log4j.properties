application.logging.appender.layout = org.apache.log4j.PatternLayout
application.logging.layout.pattern = %d{yyyy-MM-dd HH:mm:ss} [%p] %C.%M (%L): %m%n

# APPENDER
# stdout
log4j.appender.stdout = org.apache.log4j.ConsoleAppender
log4j.appender.stdout.Threshold = INFO
log4j.appender.stdout.Target = System.out
log4j.appender.stdout.layout = ${application.logging.appender.layout}
log4j.appender.stdout.layout.ConversionPattern = ${application.logging.layout.pattern}

# rolling_file
log4j.appender.rolling_file = org.apache.log4j.RollingFileAppender
log4j.appender.rolling_file.Threshold = INFO
log4j.appender.rolling_file.append = true
log4j.appender.rolling_file.File = ${application.logging.filename}
log4j.appender.rolling_file.layout = ${application.logging.appender.layout}
log4j.appender.rolling_file.layout.ConversionPattern = ${application.logging.layout.pattern}
log4j.appender.rolling_file.MaxFileSize = 1MB
log4j.appender.rolling_file.MaxBackupIndex = 3

# LOGGERS
log4j.rootLogger = INFO, stdout
log4j.logger.it.luca.pipeline = INFO, stdout
log4j.logger.org.apache.hadoop = WARN, stdout
log4j.logger.org.apache.spark = WARN, stdout
log4j.logger.org.spark_project.jetty = WARN, stdout
log4j.logger.org.apache.parquet = WARN, stdout
log4j.logger.org.datanucleus.util = WARN, stdout

# avoids Windows-related errors when ShutdownHookManager
# attempts to delete spark temporary working directory
log4j.logger.org.apache.spark.util.ShutdownHookManager = OFF
log4j.logger.org.apache.spark.SparkEnv = ERROR

# LOGGER ADDITIVITY
log4j.additivity.it.luca.pipeline = false
log4j.additivity.org.apache.hadoop = false
log4j.additivity.org.apache.spark = false
log4j.additivity.org.spark_project.jetty = false
log4j.additivity.org.apache.parquet = false
log4j.additivity.org.datanucleus.util = false